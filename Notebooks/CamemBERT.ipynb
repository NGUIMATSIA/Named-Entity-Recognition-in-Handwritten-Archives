{"cells":[{"cell_type":"markdown","metadata":{"id":"NwxOfcWCPYGP"},"source":["# Name Entity Recognition"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11159,"status":"ok","timestamp":1714208418962,"user":{"displayName":"Franki Nguimatsia","userId":"17250965153113534178"},"user_tz":-120},"id":"MlVwGhUpUa0V","outputId":"9bfb850d-54d0-4efb-8870-82f3cac84a43"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.0)\n","Collecting datasets\n","  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n","  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.4.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=c7c3e46443890601283b3e855683575993a79dcae3dfec432b11ab252e584e7f\n","  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n","Successfully built seqeval\n","Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, seqeval, datasets\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.20.3\n","    Uninstalling huggingface-hub-0.20.3:\n","      Successfully uninstalled huggingface-hub-0.20.3\n","Successfully installed datasets-2.19.0 dill-0.3.8 huggingface-hub-0.22.2 multiprocess-0.70.16 seqeval-1.2.2 xxhash-3.4.1\n"]}],"source":["!pip install transformers datasets seqeval"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":58187,"status":"ok","timestamp":1714208477141,"user":{"displayName":"Franki Nguimatsia","userId":"17250965153113534178"},"user_tz":-120},"id":"CAHtz6qVfgM6","outputId":"c2f0eeb7-901b-4af2-b457-01abaf6e7001"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting accelerate\n","  Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/297.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/297.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.22.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n","Successfully installed accelerate-0.29.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"]}],"source":["!pip install accelerate -U"]},{"cell_type":"markdown","metadata":{"id":"3CllT18NNSFl"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wepuwdkaUa0W"},"outputs":[],"source":["\n","import csv\n","import json\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import pandas as pd\n","import random\n","import shutil\n","import torch\n","from collections import Counter, defaultdict\n","from datasets import load_dataset, ClassLabel\n","from seqeval.metrics import classification_report, f1_score\n","from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n","from torch.nn.functional import cross_entropy\n","from torch.optim import AdamW\n","from torch.utils.data import DataLoader, SubsetRandomSampler\n","from transformers import (\n","    AutoTokenizer, AutoModelForTokenClassification, AutoConfig,\n","    DataCollatorForTokenClassification,\n","    TrainingArguments, Trainer, get_scheduler\n",")\n","from wordcloud import WordCloud"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ICDamGPUzLC"},"outputs":[],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VLER-_AZU61b"},"outputs":[],"source":["!ls /content/drive/MyDrive/NLP_Ensae/"]},{"cell_type":"markdown","metadata":{"id":"nZOqJvScUa0D"},"source":["## Data Processing\n","\n","As our task essentially involves named entity recognition, the first step is to format the data according to the required specifications. For instance, we will fine-tune the pre-trained Bert and Camembert models for such a task. These models take input data in the format below, so we need to preprocess our dataset to fit this format.\n","\n","```\n","{\n","  \"tokens\": [\"NGUIMATSIA\", \"TIOFACK\", \"Franki\",\"enfant\", \"etudiant\", \"25\", \"Français\", \"néant\"],\n","  \"ner_tags\": [\"B-surname\", \"I-surname\", \"B-firstname\", \"B-occupation\", \"B-age\", \"B-nationality\", \"O\"]\n","}\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T8Yh-8PGUa0K"},"outputs":[],"source":["import json\n","import pandas as pd\n","with open(\"/content/drive/MyDrive/NLP_Ensae/entities.json\", 'r') as f:\n","    data = json.load(f)\n","df = pd.DataFrame([data])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"snc_fXXIH9Xh"},"outputs":[],"source":["entity_without_indi = {\n","    0: \"O\",\n","    1: 'surname_household',\n","    2: 'firstname',\n","    3: 'occupation',\n","    4: 'link',\n","    5: 'birth_date',\n","    6: 'nationality',\n","    7: 'lob',\n","    8: 'age',\n","    9: 'civil_status',\n","    10: 'employer',\n","    11: 'observation',\n","    12: 'surname'}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rSIzSX6nUa0M"},"outputs":[],"source":["entity_map = {\n","    'Ⓟ': 'surname_household',\n","    'Ⓕ': 'firstname',\n","    'Ⓜ': 'occupation',\n","    'Ⓗ': 'link',\n","    'Ⓑ': 'birth_date',\n","    'Ⓚ': 'nationality',\n","    'Ⓘ': 'lob',\n","    'Ⓐ': 'age',\n","    'Ⓒ': 'civil_status',\n","    'Ⓔ': 'employer',\n","    'Ⓛ': 'observation',\n","    'Ⓞ': 'surname'}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X2J5TW3pUa0O"},"outputs":[],"source":["entity = { \"O\",\n","    \"B-surname_household\",\n","    \"I-surname_household\",\n","    \"B-firstname\",\n","    \"I-firstname\",\n","    \"B-occupation\",\n","    \"I-occupation\",\n","    \"B-link\",\n","    \"I-link\",\n","    \"B-birth_date\",\n","    \"I-birth_date\",\n","    \"B-nationality\",\n","    \"I-nationality\",\n","    \"B-lob\",\n","    \"I-lob\",\n","    \"B-age\",\n","    \"I-age\",\n","    \"B-civil_status\",\n","    \"I-civil_status\",\n","    \"B-employer\",\n","    \"I-employer\",\n","    \"B-observation\",\n","    \"I-observation\",\n","    \"B-surname\",\n","    \"I-surname\"\n","\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ukwRjEtUa0P"},"outputs":[],"source":["# This function process individual information into a format: {\"tokens\": tokens, \"ner_tags\": ner_tags}\n","def transform_data(data):\n","    tokens = []\n","    ner_tags = []\n","    current_entity = None\n","    current_token = ''\n","\n","    for char in data:\n","        if char in entity_map:\n","            if current_entity:\n","                tokens.append(current_token.strip())  # Add the current token\n","                ner_tags.append(entity_map[current_entity])\n","                current_token = ''  # Reset the token\n","            current_entity = char  # Update the current entity\n","        else:\n","            if current_entity:\n","                current_token += char  # Add character to the current token\n","\n","    # Add the last token\n","    if current_entity:\n","        tokens.append(current_token.strip())  # Add the last token\n","        ner_tags.append(entity_map[current_entity])\n","\n","    return {\"tokens\": tokens, \"ner_tags\": ner_tags}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6vLiAIClUa0R"},"outputs":[],"source":["# Here we process each ligne (which represent an individu) of our JSON file\n","data_set_prepo = []\n","\n","for document in data.values():\n","    individus = document.split('\\n')\n","\n","    for individu in individus:\n","        if \"idem\" in individu:\n","            line_in = transform_data(individu)\n","            keys_with_idem_values = [tag for tag, token in zip(line_in[\"ner_tags\"], line_in[\"tokens\"]) if token == \"idem\"]\n","            for tag in keys_with_idem_values:\n","                index_tag_current = line_in[\"ner_tags\"].index(tag)\n","                stop = False\n","                i = 1\n","                while not stop and i <= len(data_set_prepo):\n","                    stop = (tag in data_set_prepo[-i][\"ner_tags\"])\n","                    i += 1\n","\n","                if stop:\n","                    index_tag_previous = data_set_prepo[-(i-1)][\"ner_tags\"].index(tag)\n","                    line_in[\"tokens\"][index_tag_current] = data_set_prepo[-(i-1)]['tokens'][index_tag_previous]\n","\n","        else:\n","            line_in = transform_data(individu)\n","\n","        data_set_prepo.append(line_in)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aYgrgEH3Ua0S"},"outputs":[],"source":["for i in range(40, 45):\n","    print(data_set_prepo[i])"]},{"cell_type":"markdown","metadata":{"id":"nyxUcqbsY-_Q"},"source":["We label all tokens 'néant' as 'O' to indicate that these tokens are not part of the elements to be targeted"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rHCGW-H_Ua0T"},"outputs":[],"source":["for i in range(len(data_set_prepo)):\n","    if 'néant' in data_set_prepo[i]['tokens']:\n","        for k in range(len(data_set_prepo[i]['tokens'])):\n","            if 'néant' in data_set_prepo[i]['tokens'][k]:\n","                data_set_prepo[i]['ner_tags'][k] = 'O'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ctdT67rsUa0U"},"outputs":[],"source":["for i in range(43, 45):\n","    print(data_set_prepo[i])"]},{"cell_type":"markdown","metadata":{"id":"RJU2JnR7W3GO"},"source":["As can be seen above, an individual may have multiple names or first names. For example, the entity 'Jean Baptiste' represents the first name of the same individual. Therefore, it is necessary to reprocess the data to specify that 'Jean' is the beginning of the first name and 'Baptiste' is its continuation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1vyY8fOnHYqH"},"outputs":[],"source":["for i in range(len(data_set_prepo)):\n","  ind = data_set_prepo[i]\n","  for k in range(len(entity_without_indi)):\n","    if entity_without_indi[k] in ind[\"ner_tags\"]:\n","      index = ind[\"ner_tags\"].index(entity_without_indi[k])\n","\n","      if len(ind[\"tokens\"][index].split()) > 1 :\n","        ent =[]\n","        words = list(ind[\"tokens\"][index].split())\n","        ent.append(\"B-\" + entity_without_indi[k])\n","\n","        for _ in range(1, len(words)):\n","          ent.append(\"I-\" + entity_without_indi[k])\n","\n","        data_set_prepo[i][\"tokens\"][index:index+1] = words\n","        data_set_prepo[i][\"ner_tags\"][index:index+1] = ent\n","      elif data_set_prepo[i][\"ner_tags\"][index] != \"O\" :\n","        data_set_prepo[i][\"ner_tags\"][index] = \"B-\" + data_set_prepo[i][\"ner_tags\"][index]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RSNtbfRRRtLh"},"outputs":[],"source":["data_set_prepo[43]"]},{"cell_type":"markdown","metadata":{"id":"88PuHC_FZ1FE"},"source":["## Dataset\n","\n","We now want to convert our three JSON-L files `train.jsonl`, `validation.jsonl` and `test.jsonl` files into our raw dataset.\n","\n","First we produce a mapping of name to file path, and call `load_dataset` on it with `format=\"json\"`.\n","\n","This creates a DatasetDict object with three Datasets identified by the names specified in the `data_files` mapping."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e33HVsQ8Ua0W"},"outputs":[],"source":["\n","GS_DATA_DIR = \"/content/drive/MyDrive/NLP_Ensae/\"\n","DATA_DIR = \"data\"\n","\n","NER_FILEPATH = os.path.join(GS_DATA_DIR, \"ner_dataset.csv\")\n","\n","OUTPUT_FILEPATHS = [\n","  os.path.join(DATA_DIR, \"gmb-train.jsonl\"),\n","  os.path.join(DATA_DIR, \"gmb-valid.jsonl\"),\n","  os.path.join(DATA_DIR, \"gmb-test.jsonl\")\n","]\n","\n","BASE_MODEL_NAME = \"camembert-base\" # Other models: xlm-roberta-base, camembert-base, camembert/camembert-large, distilbert-base-cased,\n","MODEL_DIR = os.path.join(DATA_DIR, \"{:s}-gmb-ner\".format(BASE_MODEL_NAME))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QCMIQFLMUa0W"},"outputs":[],"source":["def write_output(tokens, labels, output_files, num_written):\n","  assert(len(tokens) == len(labels))\n","  rec = json.dumps({ \"tokens\": tokens, \"ner_tags\": labels })\n","  dice = random.random()\n","  if dice <= 0.8:\n","    output_files[0].write(\"{:s}\\n\".format(rec))\n","    num_written[0] += 1\n","  elif dice <= 0.9:\n","    output_files[1].write(\"{:s}\\n\".format(rec))\n","    num_written[1] += 1\n","  else:\n","    output_files[2].write(\"{:s}\\n\".format(rec))\n","    num_written[2] += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jO5hFSMuUa0X"},"outputs":[],"source":["os.makedirs(DATA_DIR, exist_ok=True)\n","output_files = [open(filepath, \"w\") for filepath in OUTPUT_FILEPATHS]\n","num_written = [0, 0, 0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-r6mAIW9Ua0X"},"outputs":[],"source":["for i in range(len(data_set_prepo)):\n","    tokens = data_set_prepo[i][\"tokens\"]\n","    labels = data_set_prepo[i][\"ner_tags\"]\n","    write_output(tokens, labels, output_files, num_written)\n","\n","\n","[output_file.close() for output_file in output_files]\n","print(num_written)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EWVy1oR0Ua0X"},"outputs":[],"source":["data_files = {\n","    \"train\": OUTPUT_FILEPATHS[0],\n","    \"validation\": OUTPUT_FILEPATHS[1],\n","    \"test\": OUTPUT_FILEPATHS[2]\n","}\n","final_dataset = load_dataset(\"json\", data_files=data_files)\n","final_dataset"]},{"cell_type":"markdown","metadata":{"id":"4MUAac77aGow"},"source":["## Distribution of Entity Types across splits\n","\n","We want to make sure that the validation and test sets look roughly similar to the training set in terms of distribution of data across the different classes. We can verify that by using the following code, which computes the distribution across the different classes across different splits.\n","\n","We see that the distribution of tags are roughly similar across splits, so validation and test sets should provide a good measure of the NER's generalization capabilities."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KOsIcUVKd6dd"},"outputs":[],"source":["tag_freqs_by_split = defaultdict(Counter)\n","for split, dataset in final_dataset.items():\n","  for ner_tags in dataset[\"ner_tags\"]:\n","    for tag in ner_tags:\n","      if tag.startswith(\"B-\"):\n","        tag = tag.replace(\"B-\", \"\")\n","        tag_freqs_by_split[split][tag] += 1\n","pd.DataFrame.from_dict(tag_freqs_by_split, orient=\"index\")"]},{"cell_type":"markdown","metadata":{"id":"PEwYMNWUxAbP"},"source":["### Class Label Mappings\n","\n","In the encoded dataset, since the input will be converted to arrays of floats, we also want to do the same thing with the labels.\n","\n","For this we construct a ClassLabel object and use its `int2str` and `str2int` methods to construct lookup tables from label name to label id and vice versa."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R8XaTKc8Ua0Y"},"outputs":[],"source":["entity_types = set()\n","for ner_tags in final_dataset[\"train\"][\"ner_tags\"]:\n","  for ner_tag in ner_tags:\n","    if ner_tag.startswith(\"B-\"):\n","      entity_types.add(ner_tag.replace(\"B-\", \"\"))\n","entity_types = sorted(list(entity_types))\n","\n","tag_names = []\n","for entity_type in entity_types:\n","  tag_names.append(\"B-{:s}\".format(entity_type))\n","  tag_names.append(\"I-{:s}\".format(entity_type))\n","tag_names.append(\"O\")\n","\n","tags = ClassLabel(names=tag_names)\n","label2id = {name: tags.str2int(name) for name in tag_names}\n","id2label = {id: tags.int2str(id) for id in range(len(tag_names))}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vh5Cel_-WPx1"},"outputs":[],"source":["#label2id, id2label"]},{"cell_type":"markdown","metadata":{"id":"kdj-7XLmacg5"},"source":["## Tokenization and Label Alignment\n","\n","We now have to deal with the subword tokenization alignment problem that come with using Transformers.\n","\n","Transformers use subword tokenization to split long words into their more common subwords, for example \"française\" could become \"franç\", \"##ai\", and \"##se\". Common subwords would be reused across different words. This helps to keep the vocabulary size down, or conversely, helps to cover a larger input vocabulary using the same size of tokenizer vocabulary.\n","\n","However, our input sequence of `tokens` is space separated (with some additional heuristics around punctuation that is already taken care of in the provided input) and our `ner_tags` are aligned with this space-separated token sequence.\n","\n","Since Transformers expect their input to be subword tokenized, the corresponding labels need to be aligned with the subword tokens as well. So for our \"française\" example, the corresponding labels will change from a single \"B-LOC\" to \"B-LOC\", \"IGN\", and \"IGN\", i.e. the trailing subwords would be ignored.\n","\n","To do the mapping, we need the Tokenizer's `word_ids()` function, that maps the original token ids onto the subword tokenized sequence, which is only available from the \"faster\" Rust-based Tokenizer implementations. Fortunately, the AutoTokenizer will choose version of the Tokenizer for us automatically. We will use this to construct an \"aligned\" version of the `ner_tags`.\n","\n","In addition, we can also take this opportunity to construct an \"encoded\" version of the GMB dataset, since we are calling the tokenizer on the input tokens anyway and as a result have access to the `input_ids`, `token_type_ids` and `attention_masks`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4TOcibxTWn13"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TNxf3XtowGjK"},"outputs":[],"source":["tokens = tokenizer(final_dataset[\"train\"][7][\"tokens\"], is_split_into_words=True).tokens()\n","tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tQO-n1uLwe1m"},"outputs":[],"source":["input = tokenizer(final_dataset[\"train\"][0][\"tokens\"], is_split_into_words=True)\n","word_ids = input.word_ids()\n","pd.DataFrame([tokens, word_ids], index=[\"tokens\", \"word_ids\"])"]},{"cell_type":"markdown","metadata":{"id":"dc2fSC9jxXQq"},"source":["We now define the `tokenize_and_align_labels` which takes a batch of records from the input raw dataset, i.e. the input `examples` object is a list of dictionaries with keys `{\"tokens\", \"ner_tags\"}`.\n","\n","The output of this function is a list of tokenized input objects, which are dictionaries with keys `{\"input_ids\", \"token_type_ids\", \"attention_masks\", \"labels\"}`. The first three keys are from the output of the tokenizer, and the last one is a numeric representation of the input `ner_tags` aligned to the subworded tokens.\n","\n","We convert the raw dataset to the encoded one by calling `map` on the raw dataset with `tokenize_and_align_labels` as the function. The `map` call is applied batch-wise, and the original keys `ner_tags` and `tokens` are removed from the encoded dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sPM3hyuHWu0c"},"outputs":[],"source":["def tokenize_and_align_labels(examples):\n","  tokenized_inputs = tokenizer(examples[\"tokens\"],\n","                               truncation=True,\n","                               is_split_into_words=True)\n","  aligned_batch_labels = []\n","  for idx, labels in enumerate(examples[\"ner_tags\"]):\n","    word_ids = tokenized_inputs.word_ids(batch_index=idx)\n","    prev_word_id = None\n","    aligned_labels = []\n","    for word_id in word_ids:\n","      if word_id is None or word_id == prev_word_id:\n","        aligned_labels.append(-100)   # IGNore tag\n","      else:\n","        aligned_labels.append(label2id[labels[word_id]])\n","      prev_word_id = word_id\n","    aligned_batch_labels.append(aligned_labels)\n","  tokenized_inputs[\"labels\"] = aligned_batch_labels\n","  return tokenized_inputs\n","\n","\n","tokens = final_dataset[\"train\"][0][\"tokens\"]\n","ner_tags = final_dataset[\"train\"][0][\"ner_tags\"]\n","aligned_labels = tokenize_and_align_labels(final_dataset[\"train\"][0:1])[\"labels\"][0]\n","len(tokens), len(ner_tags), len(aligned_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AfACB_SAW8Ru"},"outputs":[],"source":["encoded_final_dataset = final_dataset.map(tokenize_and_align_labels,\n","                                      batched=True,\n","                                      remove_columns=[\"ner_tags\", \"tokens\"])\n","encoded_final_dataset"]},{"cell_type":"markdown","metadata":{"id":"iL6Ab2ntyYpA"},"source":["## Model\n","\n","This step discovers if we have a GPU available. If we do, the model is instantiated and assigned to the GPU, otherwise it continues to process on the CPU.\n","\n","In the `AutoModelForTokenClassification.from_pretrained` call, we instantiate a `BertForTokenClassification` object from HuggingFace. This is a model with a BERT encoder and a head consisting of a `torch.nn.Dropout` and a `torch.nn.Linear` layer for classifying the output of the pre-trained BERT encoder into one of `num_labels` possible NER tags.\n","\n","__NOTE:__ remember to set the `num_labels` parameter, otherwise the model will default to a binary classifier, and you will see a hard to understand error when you do the training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R7FQzkraXNcA"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2p1fx70nXfH2"},"outputs":[],"source":["model = AutoModelForTokenClassification.from_pretrained(BASE_MODEL_NAME,\n","                                                        num_labels=len(tag_names),\n","                                                        id2label=id2label,\n","                                                        label2id=label2id)\n","model = model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"63L4M9DB0Esq"},"source":["#Training\n","We use the HuggingFace Trainer API to do the fine-tuning instead of native PyTorch.\n","\n","There are a number of advantages to using the Trainer API. First your code is less verbose and you automatically have access to best practices that are directly inbuilt into the Trainer API.\n","\n","The Trainer API works on datasets directly. We don't have to create a DataLoader explicitly.\n","\n","However, we do have to honor the expectations of the Trainer API. For example, the version of the `compute_metrics` function takes an `EvalPrediction` object rather than a pair of list of lists. So we need to learn a little more about the `Trainer` and `TrainerArguments` API but the benefits are quite good."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ftGO3VzVXvPY"},"outputs":[],"source":["num_epochs = 12\n","batch_size = 16  # larger model, lower batch size\n","logging_steps = len(encoded_final_dataset[\"train\"]) // batch_size\n","model_name = \"{:s}-gmb-ner\".format(BASE_MODEL_NAME)\n","training_args = TrainingArguments(\n","    output_dir=model_name,\n","    log_level=\"error\",\n","    num_train_epochs=num_epochs,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    evaluation_strategy=\"epoch\",\n","    save_steps=1e6,\n","    weight_decay=0.01,\n","    disable_tqdm=False,\n","    logging_steps=logging_steps\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XvrvLe9mYAHz"},"outputs":[],"source":["def align_predictions(labels_cpu, preds_cpu):\n","  # remove -100 labels from score computation\n","  batch_size, seq_len = preds_cpu.shape\n","  labels_list, preds_list = [], []\n","  for bid in range(batch_size):\n","    example_labels, example_preds = [], []\n","    for sid in range(seq_len):\n","      # ignore label -100\n","      if labels_cpu[bid, sid] != -100:\n","        example_labels.append(id2label[labels_cpu[bid, sid]])\n","        example_preds.append(id2label[preds_cpu[bid, sid]])\n","    labels_list.append(example_labels)\n","    preds_list.append(example_preds)\n","  return labels_list, preds_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PoJjk-0PYGIZ"},"outputs":[],"source":["def compute_metrics(pred):\n","  # convert logits to predictions and move to CPU\n","  preds_cpu = np.argmax(pred.predictions, axis=-1)\n","  labels_cpu = pred.label_ids\n","  labels_list, preds_list = align_predictions(labels_cpu, preds_cpu)\n","  # seqeval.metrics.f1_score takes list of list of tags\n","  return { \"f1\": f1_score(labels_list, preds_list) }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cvb7_67_YKEG"},"outputs":[],"source":["data_collator = DataCollatorForTokenClassification(tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gQCfibgSYOEM"},"outputs":[],"source":["\n","config = AutoConfig.from_pretrained(BASE_MODEL_NAME,\n","                                    num_labels=len(tag_names),\n","                                    id2label=id2label,\n","                                    label2id=label2id)\n","def model_init():\n","  return (AutoModelForTokenClassification\n","          .from_pretrained(BASE_MODEL_NAME, config=config)\n","          .to(device))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xQGbSq68YXry"},"outputs":[],"source":["trainer = Trainer(\n","    model_init=model_init,\n","    args=training_args,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    train_dataset=encoded_final_dataset[\"train\"],\n","    eval_dataset=encoded_final_dataset[\"validation\"],\n","    tokenizer=tokenizer\n",")\n","trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"G-XKZnHc1J6U"},"source":["## Evaluation\n","\n","We evaluate our trained model against the test split in two ways.\n","\n","1. Compute the precision, recall, and f1-score for each NER tag and produce the classification report.\n","2. Confusion matrix -- a more graphical and easier to understand report of model performance. We can see for which NER tags the model is doing well, and for which ones it is making mistakes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zMI74ODUYh7B"},"outputs":[],"source":["def get_label_and_predicted_tags(examples):\n","  features = [dict(zip(examples, t)) for t in zip(*examples.values())]\n","  batch = data_collator(features)\n","  input_ids = batch[\"input_ids\"].to(device)\n","  attention_mask = batch[\"attention_mask\"].to(device)\n","  labels = batch[\"labels\"].cpu().numpy()\n","  with torch.no_grad():\n","    output = trainer.model(input_ids, attention_mask=attention_mask)\n","    predicted_label = torch.argmax(output.logits, dim=-1).cpu().numpy()\n","  return {\n","    \"labels\": labels,\n","    \"predicted_labels\": predicted_label\n","  }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L9w6NTU4YoNS"},"outputs":[],"source":["id2label[-100] = \"IGN\"\n","test_labels_list, test_preds_list = [], []\n","test_dataset = encoded_final_dataset[\"test\"].map(get_label_and_predicted_tags, batched=True)\n","for test_rec in test_dataset:\n","  lp_list = [(id2label[l], id2label[p])\n","             for l, p in zip(test_rec[\"labels\"], test_rec[\"predicted_labels\"])\n","             if l != -100]\n","  test_labels_list.append([l for l, p in lp_list])\n","  test_preds_list.append([p for l, p in lp_list])\n","\n","print(classification_report(test_labels_list, test_preds_list))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K54Mogw4Y2dR"},"outputs":[],"source":["def plot_confusion_matrix(ytrue, ypreds, labels):\n","    cm = confusion_matrix(ytrue, ypreds, normalize=\"true\")\n","    fig, ax = plt.subplots(figsize=(10, 10))\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n","    disp.plot(cmap=\"Blues\", values_format=\"0.2f\", ax=ax, colorbar=False)\n","    ax.set_xticklabels(labels, rotation=90)  # Rotation des labels en abscisse\n","    plt.title(\"Normalized Confusion Matrix\")\n","    _ = plt.show()\n","\n","flat_test_labels, flat_test_preds = [], []\n","for test_labels, test_preds in zip(test_labels_list, test_preds_list):\n","  flat_test_labels.extend(test_labels)\n","  flat_test_preds.extend(test_preds)\n","tag_names.remove(\"I-birth_date\")\n","plot_confusion_matrix(flat_test_labels, flat_test_preds, tag_names)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":0}